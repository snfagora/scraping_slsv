---
title: "Parsing texts"
author: "Jae Yeon Kim"
date: "`r Sys.Date()`"
output: html_document
---

# Install pkgs 

```{r}
#remotes::install_github(c("ropensci/tabulizerjars", "ropensci/tabulizer"))
#devtools::install_github("hegghammer/daiR")
#devtools::install_github("daniel1noble/metaDigitise")

if(!require(pacman)) install.packages("pacman")
pacman::p_load(tesseract, magick, zoo, parallel, pdftools, naniar, tidyverse, here, glue, purrr, stringr, stringi, tabulizer, daiR, metaDigitise)

source(here("functions", "utils.R"))
```

# Load data 

```{r}
file_lists <- list.files(here("outputs", "nslve"))

target <- file_lists[str_detect(tolower(file_lists), "nslve") & !str_detect(tolower(file_lists), "json")]

target %>% length() # 1,286
```

# Parse report

## Test version

```{r}
# file path
## tested: target[1], target[2], target[5]
#target_text <- here("outputs", "nslve", "2018-NSLVE-Report-Kapiolani-Community-College.pdf")

target_text <- here("outputs", "nslve", target[5])

# extract text (a list object)
text <- pdftools::pdf_text(target_text)

# lower case the texts 
lower_text <- map(text, tolower)

# find the page that contains the target pattern: "registration rate"
pos <- which(str_detect(lower_text, "total student enrollment"))
     
# drop the first page (unlikely to have the registration change graph)
tp <- pos[! pos %in% c(1)]

# turn list into character vector
raw_text <- lower_text[[tp]] %>%
  str_split("\n") %>%
  unlist()

# find the start and last line (element) of the plot
first <- which(str_detect(raw_text, "   total student enrollment"))
last <- which(str_detect(raw_text, "   difference from all institutions"))

# years
years <- raw_text[(first-2):(first-1)] %>% 
  str_split(" ") %>%
  unlist() %>%
  stri_remove_empty() %>%
  unique() %>%
  as.numeric() %>%
  na.omit() # in case, there's a NA value

years <- years[order(years)]

# isolate the table frame 
table <- raw_text[first:last]

# turn the character vector into a dataframe
table_df <- table %>%
  str_trim(side = "left") %>%
  stri_remove_empty() %>%
  str_split_fixed(" {5,}", 4) %>%
  as_tibble()

# name columns
col_names <- c("names", years, "change")

colnames(table_df) <- col_names

# cleanup the dataframe
replace_weird_na <- function(x){
  
  x[x == "n/a"] <- NA
 
  return(x) 
}

final_df <- table_df %>%
  mutate(across(everything(), na_if, "n/a")) %>%
  mutate(across(everything(), na_if, "")) %>%
  mutate(across(-names, parse_number))

final_df
```

## Function 

```{r}
parse_report <- function(target) {

  message(paste("Started parsing: ", target))
  
  # file path
  ## tested: target[1], target[2], target[5]
  target_text <- here("outputs", "nslve", target)
  
  # extract text (a list object)
  text <- pdftools::pdf_text(target_text)
  
  # lower case the texts 
  lower_text <- map(text, tolower)
  
  # find the page that contains the target pattern: "registration rate"
  pos <- which(str_detect(lower_text, "total student enrollment"))
       
  # drop the first page (unlikely to have the registration change graph)
  tp <- pos[! pos %in% c(1)]
  
  # turn list into character vector
  raw_text <- lower_text[[tp]] %>%
    str_split("\n") %>%
    unlist()
  
  # find the start and last line (element) of the plot
  first <- which(str_detect(raw_text, "   total student enrollment"))
  last <- which(str_detect(raw_text, "   difference from all institutions"))
  
  # years
  years <- raw_text[(first-2):(first-1)] %>% 
    str_split(" ") %>%
    unlist() %>%
    stri_remove_empty() %>%
    unique() %>%
    as.numeric() %>%
    na.omit() # in case, there's a NA value
  
  years <- years[order(years)]
  
  # isolate the table frame 
  table <- raw_text[first:last]
  
  # turn the character vector into a dataframe
  table_df <- table %>%
    str_trim(side = "left") %>%
    stri_remove_empty() %>%
    str_split_fixed(" {5,}", 4) %>%
    as_tibble()
  
  # name columns
  col_names <- c("names", years, "change")
  
  colnames(table_df) <- col_names
  
  # cleanup the dataframe
  replace_weird_na <- function(x){
    
    x[x == "n/a"] <- NA
   
    return(x) 
  }
  
  final_df <- table_df %>%
    mutate(across(everything(), na_if, "n/a")) %>%
    mutate(across(everything(), na_if, "")) %>%
    mutate(across(-names, parse_number))
  
  # add the file source
  final_df <- final_df %>%
    mutate(source = target)

  message(paste("Completed parsing: ", target))
    
  return(final_df)
}
```

## Map 

```{r}
# test
walk(1:300, ~parse_report(target[.]))
```

```{r}
# Map file lists to parse text 
parsed_df <- map(target, safely(parse_report))

filtered_df <- parsed_df %>%
  map("result") %>%
  compact() %>%
  reduce(bind_rows)

head(filtered_df)

filtered_df$source %>% unique() %>% length()
# 727 scraped 
# 559 remained
# 1286 - filtered_df$source %>% unique() %>% length()

filtered_df
```

# Export the file

```{r}
write_csv(filtered_df, here("outputs", "report_df.csv"))
```