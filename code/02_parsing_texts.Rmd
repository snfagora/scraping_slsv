---
title: "Parsing texts"
author: "Jae Yeon Kim"
date: "`r Sys.Date()`"
output: html_document
---

# Install pkgs 

```{r}
if(!require(pacman)) install.packages("pacman")
pacman::p_load(tesseract, magick, zoo, parallel, pdftools, naniar, tidyverse, here, glue, purrr, stringr, lubridate)

source(here("functions", "utils.R"))
```

# Load data 

```{r}
file_lists <- list.files(here("outputs"))
length(file_lists) # 1,231

file_lists <- file_lists[str_detect(file_lists, ".pdf")]
length(file_lists) # 1,230 (removed one file)

year_outs <- map_dfr(file_lists, extract_years)

year_outs %>%
  pivot_longer(c("begin", "end"),
               names_to = "time",
               values_to = "year") %>%
  group_by(year) %>%
  count() %>%
  ggplot(aes(x = ymd(sprintf("%d-01-01", year)), y = n)) +
    geom_line() +
    labs(x = "Year",
         y = "Count",
         title = "The years covered by Action Plans",
         subtitle = glue("The number of reports: {scales::label_comma(, accuracy = 1)(length(file_lists))}"))

ggsave(here("figures", "action_plan.png"))
```

# Parse text 

## Test version

```{r}
target_text <- here("outputs", file_lists[1])

text <- pdftools::pdf_text(target_text) 

# collapse list elements into one vector 
text <- paste(text, collapse = "\n") 

# replace special characters with one space
text <- gsub("[[:punct:]]", " ", text)

# remove excess white space 
text <- gsub("\\s+", " ", text)

# lower case
text <- tolower(text)

# a dataframe that combines text and filename vectors

df <- tibble(text = text,
             name = file_lists[1])
```

## Function 

```{r}
parse_text <- function(target) {

  target_text <- here("outputs", target)

  text <- pdftools::pdf_text(target_text) 

  # collapse list elements into one vector 
  text <- paste(text, collapse = "\n") 

  # replace special characters with one space
  text <- gsub("[[:punct:]]", " ", text)

  # remove excess white space 
  text <- gsub("\\s+", " ", text)

  # lower case
  text <- tolower(text)

  # a dataframe that combines text and filename vectors
  df <- tibble(text = text,
               name = target)
  
  return(df)
}
```

## Map 

```{r}
# Map file lists to parse text 
parsed_df <- map_dfr(file_lists, parse_text)

# Identify and filter failed PDF parsed results 
test_text <- function(i) {
  
  out <- substr(parsed_df$text[i], start = 1 , stop = 20)
  
  return(out)
  
  }

parsed_text <- purrr::map_chr(seq(parsed_df$text), test_text)

length(parsed_text) # 1,230

log_filters <- !(str_count(parsed_text) %in% c(1,0))

filtered_df <- parsed_df[log_filters,] # 1,226
```

# Export the file

```{r}
write_csv(filtered_df, here("outputs", "filtered_df.csv"))
#write_rds(filtered_df, here("outputs", "filtered_df.rds"))
```