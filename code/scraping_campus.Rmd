---
title: "Scraping campus"
author: "Jae Yeon Kim"
date: "`r Sys.Date()`"
output: html_document
---

# Install pkgs 

```{r}
if (!require(pacman)) install.packages("pacman")

pacman::p_load(httr, purrr, here, tidyverse,
               stringr, glue, rvest, 
               jsonlite, 
               xml2)

source(here("functions", "utils.R"))
```

# Parse websites

## Extract links from the main webpage 

```{r}
# target URL (the main webpage)

url <- "https://allinchallenge.org/participating-campuses/"

# extract links

hrefs <- extract_url(url)
```

## Filter the relevant links and find the PDF links

```{r}
# Relevant links

rev_links <- hrefs[str_detect(hrefs, "/campuses/")]

# Relevant PDF links (only Action plans))

action_links <- map_dfr(rev_links, extract_pdf)

action_links <- action_links %>%
  filter(!is.na(pdf_link))

write_csv(action_links, here("outputs", "action_links.csv"))
```

# Download PDF files

```{r}
action_links$college %>% unique() %>% length() # 625 college campuses

action_links <- action_links %>%
  mutate(dests = glue("{here('outputs', action_links$pdf_link)}"), 
         dests = str_remove_all(dests, "https://allinchallenge.org/wp-content/uploads/"))

# Bulk download files
Map(function(u, d) safely(download.file(u, d, mode="wb")), action_links$pdf_link, action_links$dests)
```

```{r}
# Check the number of PDF files

str_detect(list.files(here("outputs")), "pdf") %>% sum() # 1,230 pdf files
```